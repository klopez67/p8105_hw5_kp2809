---
title: 'Homework 5: Iteration'
author: "Kimberly Lopez"
date: "2024-11-06"
output: github_document
---
```{r warning=FALSE}
library(tidyverse)
library(rvest)
library(broom)
library(dplyr)
library(patchwork)
library(purrr)

set.seed(5)
```

# Problem 1 

Birthday problem 

```{r eval=FALSE}
group_sizes=  2:50
simulations=  10000
results=  numeric(
  length(group_sizes))
```

```{r eval=FALSE}

bday_dup= function(n) {
  
  birthdays =  sample(1:365, n, replace = TRUE)  
  return(
    any(duplicated(birthdays))) 
}

```

For loop across all 
```{r eval=FALSE}

for (i in (group_sizes)) {
  n = group_sizes[i]
  
  duplicates_count=  sum(replicate(simulations, bday_dup(n)))
  results[i]=  duplicates_count / simulations
  
}|> 
  mutate(
    data.frame(
  group_size = group_sizes,
  probability = results))

as_tibble(results)
```


Plot the results


# Problem 2

For one randomly generated sample of n=30, with mean = 0, and sd= 5, run a t.test() and return the estimate & p-value: 
```{r}
sim_data = 
    tibble(
    x = rnorm (30, mean= 0, sd= 5))

t.test(sim_data)|>
  broom::tidy()|>
  select(estimate,p.value)
```


Function for performing a one-sample t-test and returning the estimate & p-value:
```{r}

sim_t_test = function ( n , mu= 0, sigma =5) {
  
  sim_data = 
    tibble(
    x = rnorm (n, mean= mu, sd= sigma))
  
  sim_data|>
    t.test()|>
    broom::tidy()|>
    select(estimate, p.value)
  
}

sim_t_test(n=30)
```
For 5000 datasets: when mu=1
```{r}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_t_test(n=30)
}

sim_results_mu1 = bind_rows(output)

head(sim_results_mu1)
```

Function for calculating power: known as the proportion of times the null was rejected 

- Use after binding all simulations 
```{r}

sim_power = function ( data) {
  
  data|>
   filter(p.value < 0.05) |>
  summarize(
    power = n() / nrow(data), 
    mu = unique(mu))
  
}

```


Function that run the 5000 sample for loop updating mu for mu= 1,2,3,4,5,6 
```{r}
sim_results=list()

for (mu_val in 1:6) {
  

  output= vector("list", 5000)
  for (i in 1:5000) {
    output[[i]] = sim_t_test(n = 30, mu = mu_val)
  }
  
  sim_results[[mu_val]]= bind_rows(output) |>
    mutate(mu = mu_val)  
}

```

```{r}
summary(sim_results)
```


```{r}
output= map_dfr(sim_results, sim_power)|>
  bind_rows()

output 
output|>
  ggplot(aes(x=mu, y = power))+
  geom_point()
```

The scatter plot of the power of the test associated with a given mu value increases as mu increases. With a fixed sample size of n=30 and standard deviation = 5, increasing mu increases the observed power of the test. This makes sense as the larger values of mu create a greater difference from the null hypothesis, making it easier to detect a true effect.


Make a plot showing the average estimate of ðœ‡Ì‚ on the y axis and the true value of ðœ‡on the x axis. 

```{r}
all_mu_hats= 
  sim_results|>
  bind_rows()|>
  group_by(mu)|>
  summarize(
    mu_hat= mean(estimate))|>
  ggplot(aes(x=mu, y=mu_hat))+
  geom_point()

all_mu_hats
```

Make a second plot (or overlay on the first) the average estimate of ðœ‡Ì‚ only in samples for which the null was rejected on the y axis and the true value of ðœ‡on the x axis. 

```{r}
sim_results|>
  bind_rows()|>
  group_by(mu)|>
  filter(p.value < 0.05) |>
  summarize(
    mu_hat= mean(estimate))|>
  ggplot(aes(x=mu, y=mu_hat))+
  geom_point()+ 
  geom_smooth(se=FALSE)+
  labs(
    title= "Samples where the null was rejected"
  )+
  all_mu_hats+ 
  geom_smooth(se=FALSE)+ 
  labs(
    title= "For all samples")
```


Is the sample average of ðœ‡Ì‚ across tests for which the null is rejected approximately equal to the true value of ðœ‡? Why or why not?

The mu estimate for which the null is reject is not always approximately equal to the true value of mu when the null is rejected. When the null is rejected, there is significant evidence that the sample deviates from the true value of mu under the null hypothesis. As shown by the left plot, this can lead to a slight overestimation or underestimation of the true mu value for when mu values are smaller. This may also be due to the lower power observed among the smaller mu values from plot 1. 


# Problem 3 

The homicide data is data The Washington Post has gathered data on homicides in large U.S. cities. 
```{r}
homicide_data = 
  read.csv("homicide-data.csv")|>
  janitor::clean_names()|>
  mutate(
    reported_date= as.character(reported_date),                  
    reported_date = as.Date(reported_date, format = "%Y%m%d") 
  )

```

The homicide_data is made of `r nrow(homicide_data)` entries, each of which describes a homicide, and `r ncol(homicide_data)` columns. Each entry has data on `r colnames(homicide_data)`. There are `r nrow(unique(homicide_data[c("city", "state")]))` cities included where homicides occurred in the US from 
`r format(min(homicide_data[["reported_date"]], na.rm = TRUE), "%Y")` to `r format(max(homicide_data[["reported_date"]], na.rm = TRUE), "%Y")`. 

**Although the article mentions 50 cities, the city of Tulsa is counted twice in unique cities, but there are two cities named Tulsa: one in Ok and another in AL. When I google Tulsa city in Alabama, another city comes up spelled " Tuscaloosa". I am uncertain if the article counted all Tulsa city homicides for one state or not. Since the data file has not specified if the city name was misspelled, I will treat distinct city by city name and state** 


Create a city_state variable(e.g. â€œBaltimore, MDâ€) and then summarize within cities to obtain the total number of homicides and the number and the number of unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).
```{r}
homicide_data = 
  homicide_data |> 
  mutate(
    city_state = paste(city,state, sep=", "))

homicide_city_stats = 
  homicide_data|>
  group_by(city_state)|>
  summarize( 
    tot_homicides = n(), 
    unsolved_homicides = 
      sum(
      str_detect(disposition, "Closed without arrest") | str_detect(disposition, "Open/No arrest"))
    )

head(homicide_city_stats)
```
The `homicide_city_stats` dataframe has `r nrow(homicide_city_stats)` entries, each of which describes the stats of a city, and `r ncol(homicide_city_stats)` columns. Each entry has data on `r colnames(homicide_city_stats)`.


For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore_data= 
  homicide_city_stats|>
  filter(city_state =="Baltimore, MD")

baltimore_prop_test=
  prop.test(x= as.numeric(baltimore_data[3]), as.numeric(baltimore_data[2]))

baltimore_prop_test|>
  broom::tidy()|>
  select(estimate,conf.low,conf.high)
```


Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a â€œtidyâ€ pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
city_stat_nest= 
  homicide_city_stats|>
  nest( data = tot_homicides:unsolved_homicides)

city_stat_nest|> 
  group_by(city_state)|>
  mutate(
    prop_test = map(data, \(x) prop.test(x= x[["unsolved_homicides"]], n= x[["tot_homicides"]])),
    tidy_results = map(prop_test,broom::tidy))|>
  unnest(tidy_results) |>
  select(estimate, conf.low, conf.high)
```

Doing the same thing but with function `map2(x, y, .function` instead:
```{r}

city_stats= 
  city_stat_nest |>
  group_by(city_state) |>
  mutate(
    prop_test = map2(
      .x = map(data, ~.x[["unsolved_homicides"]]),  
      .y = map(data, ~.x[["tot_homicides"]]),     
      ~prop.test(x = .x, n = .y)),
    tidy_results = map(prop_test, broom::tidy)  ) |>
  unnest(tidy_results) |>
  select(estimate, conf.low, conf.high)

head(city_stats)
```

Original for loop method: same results as mapping methods 
```{r}
city_prop_results = list()


for (city in unique(homicide_city_stats[["city_state"]])){
  
  city_data= 
  homicide_city_stats|>
  filter(city_state ==city)
  
  
  city_prop_test=
  prop.test(x= as.numeric(city_data[["unsolved_homicides"]]),
            n= as.numeric(city_data[["tot_homicides"]]))
  
  city_results= 
    city_prop_test|>
    broom::tidy()|>
    select(estimate, conf.low, conf.high)|>
    mutate(
      city_state= city
    )
  
  city_prop_results[[city]] = city_results
}


all_cities_results = 
  bind_rows(city_prop_results)|>
  relocate(city_state)

head(all_cities_results)
```
I first started using a forloop method to create this dataframe, and then was able to do it using the `purr::map1` method and `purr::map2` function. I will continue with using the dataframe from the map2 function, but essentially came to create the same dataframes with all 3 methods that show `r nrow(city_stats)` entries, and `r ncol(city_stats)` columns. Each row has data on `r colnames(city_stats)` for all `r nrow(city_stats)` cities.

**Again there are 51 unique cities rather than the 50 reported in the article based on the city name and state ( there are 2 different "Tulsa" cities)**


Created a plot that shows the estimates and CIs for each city using geom_errorbar for a way to add error bars based on the upper and lower limits. 

```{r}
city_stats|>
  ggplot(aes(x=reorder(city_state, estimate), y=estimate))+
  geom_point(alpha=.4)+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ 
  labs( title= "Proportion Estimate of Unsolved Homicides by City", 
        x= "City", 
        y="Proportion Estimate of Uncolved Homicides")
```




